{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient descent</b>, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the  direction of the negative of the gradient. <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. The algorithm of gradient descent can be outlined as follows:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; choose $\\alpha_k$ to minimize $f(x_k+\\alpha_k s_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha_k s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's find a local minimum for the function $f(x) = x^3-2x^2+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3-2*x**2+2\n",
    "\n",
    "x = np.linspace(-1,2.5,100000)\n",
    "plt.plot(x, f(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of f, often called $f'$ or $f\\_prime$ is $f'(x) = 3x^2-4x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(x):\n",
    "    return 3*x**2-4*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative indicates the rate of change in the function. The larger the derivative, the more the function is changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x));\n",
    "plt.plot(x, f_prime(x)+1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is used in determining the minimal points in the function. We can see from plot above that our local minimum is gonna be near around 1.4 or 1.5 (on the x-axis), but let's pretend that we don't know that, so we set our starting point (arbitrarily, in this case) at $x_0 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 2\n",
    "n_k = 0.1\n",
    "precision = 0.0001\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "plt.plot(x, f(x));\n",
    "plt.scatter(x_list, y_list, c=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum, we will subtract the derivative from the old point, multiplied by a learning rate parameter $n_k$. This new point will have moved away from the positive change of the function, bringing us closer to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = x_new\n",
    "s_k = -f_prime(x_old)\n",
    "x_new = x_old + n_k * s_k\n",
    "x_list.append(x_new)\n",
    "y_list.append(f(x_new))\n",
    "plt.plot(x, f(x));\n",
    "plt.scatter(x_list, y_list, c=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this process until our point doesn't move very much (below a parameter threshold $precision$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below show the route that was taken to find the local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x,f(x), c=\"b\")\n",
    "plt.scatter(x_list,y_list,c=\"r\")\n",
    "plt.plot(x_list,y_list,c=\"r\")\n",
    "plt.plot(x,f(x), c=\"b\")\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x,f(x), c=\"b\")\n",
    "plt.scatter(x_list,y_list,c=\"r\")\n",
    "plt.plot(x_list,y_list,c=\"r\")\n",
    "plt.xlim([1.2,2.1])\n",
    "plt.ylim([0,3])\n",
    "plt.title(\"Gradient descent (zoomed in)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider an example which is a little bit more complicated. Consider a simple linear regression where we want to see how the temperature affects the noises made by crickets. We have a data set of cricket chirp rates at various temperatures. First we'll load that data set in and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = np.loadtxt('figures/SGD_data.txt', delimiter=',')\n",
    " \n",
    "#Plot the data\n",
    "plt.scatter(data[:, 0], data[:, 1], marker='o', c='b')\n",
    "plt.title('cricket chirps vs temperature')\n",
    "plt.xlabel('chirps/sec for striped ground crickets')\n",
    "plt.ylabel('temperature in degrees Fahrenheit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the equation of the straight line $h_\\theta(x) = \\theta_0 + \\theta_1 x$ that best fits our data points. The function that we are trying to minimize in this case is:\n",
    "\n",
    "$J(\\theta_0,\\theta_1) = {1 \\over 2m} \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)^2$\n",
    "\n",
    "In this case, our derivative will be defined in two dimensions. This is called a \"gradient\" because it has multiple components:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)$\n",
    "\n",
    "Below, we set up our function for h, J and the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lambda theta_0,theta_1,x: theta_0 + theta_1*x\n",
    "\n",
    "def J(x,y,m,theta_0,theta_1):\n",
    "    returnValue = 0\n",
    "    for i in range(m):\n",
    "        returnValue += (h(theta_0,theta_1,x[i])-y[i])**2\n",
    "    returnValue = returnValue/(2*m)\n",
    "    return returnValue\n",
    "\n",
    "def grad_J(x,y,m,theta_0,theta_1):\n",
    "    returnValue = np.array([0.,0.])\n",
    "    for i in range(m):\n",
    "        returnValue[0] += (h(theta_0,theta_1,x[i])-y[i])\n",
    "        returnValue[1] += (h(theta_0,theta_1,x[i])-y[i])*x[i]\n",
    "    returnValue = returnValue/(m)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load our data into the x and y variables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "m = len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run our gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_old = np.array([0.,0.])\n",
    "theta_new = np.array([1.,1.]) # The algorithm starts at [1,1]\n",
    "n_k = 0.001 # step size\n",
    "precision = 0.01\n",
    "num_steps = 0\n",
    "s_k = float(\"inf\")\n",
    "\n",
    "while np.linalg.norm(s_k) > precision:\n",
    "    num_steps += 1\n",
    "    theta_old = theta_new\n",
    "    s_k = -grad_J(x,y,m,theta_old[0],theta_old[1])\n",
    "    theta_new = theta_old + n_k * s_k\n",
    "\n",
    "print(\"Local minimum occurs where:\")\n",
    "print(\"theta_0 =\", theta_new[0])\n",
    "print(\"theta_1 =\", theta_new[1])\n",
    "print(\"This took\",num_steps,\"steps to converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's get the actual values for $\\theta_0$ and $\\theta_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0,21,1000)\n",
    "plt.scatter(data[:, 0], data[:, 1], marker='o', c='b')\n",
    "plt.plot(xx,h(theta_new[0],theta_new[1],xx))\n",
    "plt.xlim([13,21])\n",
    "plt.ylim([65,95])\n",
    "plt.title('cricket chirps vs temperature')\n",
    "plt.xlabel('chirps/sec for striped ground crickets')\n",
    "plt.ylabel('temperature in degrees Fahrenheit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the method above we need to calculate the gradient in every step of our algorithm. In the example with the crickets, this is not a big deal since there are only 15 data points. But imagine that we had 10 million data points. If this were the case, it would certainly make the method above far less efficient.\n",
    "\n",
    "In machine learning, the algorithm above is often called <b>batch gradient descent</b> to contrast it with <b>mini-batch gradient descent</b> (which we will not go into here) and <b>stochastic gradient descent</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, in batch gradient descent, we must look at every example in the entire training set on every step (in cases where a training set is used for gradient descent). This can be quite slow if the training set is sufficiently large. In <b>stochastic gradient descent</b>, we update our values after looking at <i>each</i> item in the training set, so that we can start making progress right away. Recall the linear regression example above. In that example, we calculated the gradient for each of the two theta values as follows:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)$\n",
    "\n",
    "Where $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Then we followed this algorithm (where $\\alpha$ was a non-adapting stepsize):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>end for</b>\n",
    "\n",
    "When the sample data had 15 data points as in the example above, calculating the gradient was not very costly. But for very large data sets, this would not be the case. So instead, we consider a stochastic gradient descent algorithm for simple linear regression such as the following, where m is the size of the data set:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Randomly shuffle the data set <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>for</b> i = 1 to m <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\begin{bmatrix}\n",
    " \\theta_{1} \\\\ \n",
    " \\theta_2 \\\\ \n",
    " \\end{bmatrix}=\\begin{bmatrix}\n",
    " \\theta_1 \\\\ \n",
    " \\theta_2 \\\\ \n",
    " \\end{bmatrix}-\\alpha\\begin{bmatrix}\n",
    " 2(h_\\theta(x_i)-y_i) \\\\ \n",
    " 2x_i(h_\\theta(x_i)-y_i) \\\\ \n",
    " \\end{bmatrix}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>end for</b> <br> \n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>\n",
    "\n",
    "Typically, with stochastic gradient descent, you will run through the entire data set 1 to 10 times (see value for k in line 2 of the pseudocode above), depending on how fast the data is converging and how large the data set is.\n",
    "\n",
    "With batch gradient descent, we must go through the entire data set before we make any progress. With this algorithm though, we can make progress right away and continue to make progress as we go through the data set. Therefore, stochastic gradient descent is often preferred when dealing with large data sets.\n",
    "\n",
    "Unlike gradient descent, stochastic gradient descent will tend to oscillate <i>near</i> a minimum value rather than continuously getting closer. It may never actually converge to the minimum though. One way around this is to slowly decrease the step size $\\alpha$ as the algorithm runs. However, this is less common than using a fixed $\\alpha$.\n",
    "\n",
    "Let's look at another example where we illustrate the use of stochastic gradient descent for linear regression. In the example below, we'll create a set of 500,000 points around the line $y = 2x+17+\\epsilon$, for values of x between 0 and 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x*2+17+np.random.randn(len(x))*10\n",
    "\n",
    "x = np.random.random(50000)*100\n",
    "y = f(x) \n",
    "m = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's randomly shuffle around our dataset. Note that in this example, this step isn't strictly necessary since the data is already in a random order. However, that obviously may not always be the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "x_shuf = []\n",
    "y_shuf = []\n",
    "index_shuf = list(range(len(x)))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    x_shuf.append(x[i])\n",
    "    y_shuf.append(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll setup our h function and our cost function, which we will use to check how the value is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lambda theta_0,theta_1,x: theta_0 + theta_1*x\n",
    "cost = lambda theta_0,theta_1, x_i, y_i: 0.5*(h(theta_0,theta_1,x_i)-y_i)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run our stochastic gradient descent algorithm. To see it's progress, we'll take a cost measurement at every step. Every 10,000 steps, we'll get an average cost from the last 10,000 steps and then append that to our cost_list variable. We will run through the entire list 10 times here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_old = np.array([0.,0.])\n",
    "theta_new = np.array([1.,1.]) # The algorithm starts at [1,1]\n",
    "n_k = 0.000005 # step size\n",
    "\n",
    "iter_num = 0\n",
    "s_k = np.array([float(\"inf\"),float(\"inf\")])\n",
    "sum_cost = 0\n",
    "cost_list = []\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(m):\n",
    "        iter_num += 1\n",
    "        theta_old = theta_new\n",
    "        s_k[0] = (h(theta_old[0],theta_old[1],x[i])-y[i])\n",
    "        s_k[1] = (h(theta_old[0],theta_old[1],x[i])-y[i])*x[i]\n",
    "        s_k = (-1)*s_k\n",
    "        theta_new = theta_old + n_k * s_k\n",
    "        sum_cost += cost(theta_old[0],theta_old[1],x[i],y[i])\n",
    "        if (i+1) % 10000 == 0:\n",
    "            cost_list.append(sum_cost/10000.0)\n",
    "            sum_cost = 0   \n",
    "            \n",
    "print(\"Local minimum occurs where:\")\n",
    "print(\"theta_0 =\", theta_new[0])\n",
    "print(\"theta_1 =\", theta_new[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our values for $\\theta_0$ and $\\theta_1$ are close to their true values of 17 and 2.\n",
    "\n",
    "Now, we plot our cost versus the number of iterations. As you can see, the cost goes down quickly at first, but starts to level off as we go through more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.arange(len(cost_list))*100\n",
    "plt.plot(iterations,cost_list)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Download ``03_sgd_iris.py`` from the course website. This uses one versus all classification, which means a single classifier is trained for each class in the data. The points are then classified based on a confidence score from each classifier. Identify the three classifiers in the datasets. What are their values? What is the classification score acheived with them? What if you train on all 4 features of the data?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features.\n",
    "X = iris.data[:,:2]\n",
    "y = iris.target\n",
    "colors = \"bry\"\n",
    "\n",
    "# shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# standardize\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "clf = SGDClassifier(max_iter=100).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired, edgecolor='black', s=20)\n",
    "plt.title(\"Decision surface of multi-class SGD\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot the three one-against-all classifiers\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "coef = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "\n",
    "def plot_hyperplane(c, color):\n",
    "    def line(x0):\n",
    "        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "\n",
    "    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "             ls=\"--\", color=color)\n",
    "\n",
    "\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    plot_hyperplane(i, color)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
